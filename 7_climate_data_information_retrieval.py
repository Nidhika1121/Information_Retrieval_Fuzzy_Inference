# -*- coding: utf-8 -*-
"""7.   Climate Data  Information Retrieval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IFSHJXDkh1dm9cvWvuCgow22Eho1DZhy
"""

from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/IR"
  

!ls /content/drive/MyDrive/IR  > /content/drive/MyDrive/b.txt

climate_files = open("/content/drive/MyDrive/b.txt")  
 
for file_i in climate_files:
  file_i_path = "/content/drive/MyDrive/IR" + "/" +file_i 
  print( file_i_path )

"""Word2Vec In buit similarity measure"""

pip install nltk

import nltk
nltk.download([
     "state_union",
     "vader_lexicon",
     "stopwords",
])

nltk.download('all')

"""# Analysis of files"""

import gensim
from gensim.models import Word2Vec
from nltk.tokenize import sent_tokenize, word_tokenize
 
globaWords = "climate change "
textTrainingInput = []
climateFolder = open("/content/fileList4.txt")  
climateFile = climateFolder.readlines()
print(climateFile)
stopwords = nltk.corpus.stopwords.words("english")

def readfiles(climateFile):
  numSentences = 0
  numWords = 0
  numWords_nonStopWords = 0
  wordList = []
  localWords = " "
  dict_file_details_sentences = {}
  dict_file_details_words = {}
  allSentences = []
  document_text = {}
  sentInDocument = ""

  for climateFileName in climateFile:
    print("File name: ", climateFileName)
    fullFileName = "/content/drive/MyDrive/IR/" + climateFileName
    print("Full file name: ", fullFileName)
    
    fileNameObj = open(fullFileName.strip())    
    sentencesInTraining = fileNameObj.read()
     
    num_local_Sentences = 0
    num_local_words = 0
    num_local_words_nonStopWords = 0
    sentInDocument= ""

    for sentence in sent_tokenize(sentencesInTraining):
      numSentences +=1
      num_local_Sentences += 1
      num_local_words += 1
      partTraningInput = []
      print("sentence ", sentence)
      allSentences.append(sentence)
      for word in word_tokenize(sentence):
        numWords += 1
        num_local_words +=1
        sentInDocument = sentInDocument + " " + word.lower()
        if word not in stopwords:
          numWords_nonStopWords += 1
          num_local_words_nonStopWords += 1
          partTraningInput.append(word.lower())
          textTrainingInput.append(partTraningInput)
          wordList.append(word.lower())
          localWords += word.lower()  
          localWords += " "
    print("Statistics of File: ", fullFileName)
    print(len(partTraningInput))
    print("Number Sentences: ", num_local_Sentences)
    print("Number of  Words: ", num_local_words)
    print("Number of words non-stop words", num_local_words_nonStopWords) 
    dict_file_details_words[climateFileName] = [num_local_words_nonStopWords , num_local_Sentences]
    document_text[fullFileName.strip()] = sentInDocument

  print("Statistics of Corpus: ")
  print("Length if Input", len(textTrainingInput))
  print("Number of total sentences: ", numSentences)
  print("Number of words: ", numWords) 
  print("Number of non stop words", numWords_nonStopWords)

  #print(dict_file_details_words)
  #print(dict_file_details_sentences)
  return localWords, wordList, allSentences, document_text
  

globaWords, wordList, allSentences, document_text = readfiles(climateFile)

 
model_bog = gensim.models.Word2Vec(textTrainingInput,  window = 5)

fileNames = []
docs_vector = []

for file in document_text:
  fileNames.append(file)
  docs_vector.append(document_text[file])

print(docs_vector)

import pandas as pd 
from sklearn.feature_extraction.text import TfidfVectorizer

#read the documents
vecTfidf = TfidfVectorizer(analyzer='word',stop_words= 'english')
tf_idf =  vecTfidf.fit_transform(docs_vector)

tfidf_features = vecTfidf.get_feature_names()

tf_idfArray = pd.DataFrame(tf_idf.toarray())
print(pd.DataFrame( tf_idfArray))
print(tfidf_features)

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

queryText = ["No country today is immune from the impacts of climate change."]
queryVector # define transformation
print(queryVector)
queryVectorArray = queryVector.toarray()
one_d_Array = queryVectorArray[0]

import numpy as np
from numpy.linalg import norm
 
 
def cosinesim(array1, array2):  
  # compute cosine similarity
  cosine = np.dot(array1,array2)/(norm(array1)*norm(array2))
  #print("Cosine Similarity:", cosine)
  return cosine

import numpy 

maxLoc = 0
cos_Sim_Query = []
for i in range(66):
  array_tfidf_ = (tf_idf[i]).toarray()
  cos_Sim_Query.append( cosinesim(array_tfidf_, one_d_Array)[0])

! pip install scikit-fuzzy

import numpy as np
import skfuzzy as fuzz
from skfuzzy import control as ctrl

# Antecedent/Consequents functions
similarity_query = ctrl.Antecedent(np.arange(0, 1, .1), 'similarity_query')
sentiment_score = ctrl.Antecedent(np.arange(0, 10, 1), 'sentiment_score')
rank = ctrl.Consequent(np.arange(0, 10, 1), 'rank')

 
similarity_query['low'] = fuzz.trimf(similarity_query.universe, [0, 0, 0.3])
similarity_query['average'] = fuzz.trimf(similarity_query.universe, [0.1, 0.5, 0.7])
similarity_query['high'] = fuzz.trimf(similarity_query.universe, [0.5, 0.8, 1])

 
sentiment_score['low'] = fuzz.trimf(sentiment_score.universe, [0, 0, 2])
sentiment_score['medium'] = fuzz.trimf(sentiment_score.universe, [0 , 4, 6])
sentiment_score['high'] = fuzz.trimf(sentiment_score.universe, [6, 9, 10])

 
rank['low'] = fuzz.trimf(rank.universe, [0, 0, 5])
rank['medium'] = fuzz.trimf(rank.universe, [1, 5, 7])
rank['high'] = fuzz.trimf(rank.universe, [5, 8, 10])


similarity_query['average'].view()
sentiment_score.view()
rank.view()


rule1 = ctrl.Rule(similarity_query['low'] | sentiment_score['low'], rank['low'])
rule2 = ctrl.Rule(similarity_query['average'] | sentiment_score['medium'], rank['medium'])
rule3 = ctrl.Rule(sentiment_score['high'] | similarity_query['high'], rank['high'])
rule4 = ctrl.Rule(similarity_query['high'] & sentiment_score['low'], rank['medium'])
rule5 = ctrl.Rule(similarity_query['high'] & sentiment_score['medium'], rank['medium'])
rule6 = ctrl.Rule(similarity_query['average'] & sentiment_score['low'], rank['medium'])
rule6 = ctrl.Rule(similarity_query['average'] & sentiment_score['low'], rank['medium'])
rule7 = ctrl.Rule(similarity_query['high'] & sentiment_score['low'], rank['high'])
rule8 = ctrl.Rule(similarity_query['low'] & sentiment_score['high'], rank['high'])
rule9 = ctrl.Rule(similarity_query['low'] & sentiment_score['medium'], rank['medium'])
rule10 = ctrl.Rule(similarity_query['low'] & sentiment_score['low'], rank['low'])

rule1.view()

rankFIS = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5, rule6, rule7, rule8, rule9, rule10])
rankFIS = ctrl.ControlSystemSimulation(rankFIS)

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

queryText = ["No country today is immune from the impacts of climate change."]
queryVector = vecTfidf.transform(queryText)
 
queryVectorArray = queryVector.toarray()
test_document = np.array([tf_idfArray[1]])
one_d_Array = queryVectorArray[0]

cos_Sim_Query = []

array_tfidf_ = (tf_idf[i]).toarray()
#cos_Sim_Query.append( cosinesim(array_tfidf_, one_d_Array)[0])

from textblob import TextBlob

fileNames = []
docs_vector = []
sentiment_doc= []

for file in document_text:
  fileNames.append(file)
  docs_vector.append(document_text[file])
  sentimentObj = TextBlob(document_text[file])
  print(file)
  print(sentimentObj.sentiment.polarity)
  sentiment_doc.append(sentimentObj.sentiment.polarity)

import numpy 

maxLoc = 0
cos_Sim_Query = []
for i in range(66):
  array_tfidf_ = (tf_idf[i]).toarray()
  cos_Sim_Query.append( cosinesim(array_tfidf_, one_d_Array)[0])

print(cos_Sim_Query[1])

rankFIS_Results = []

for i in range(66):
  rankFIS.input['similarity_query'] = cos_Sim_Query[i]  
  rankFIS.input['sentiment_score'] = sentiment_doc[i]   
  rankFIS.compute()
  print ("the answer is") 
  print(rankFIS.output['rank'])
  rank.view(sim=rankFIS)
  rankFIS_Results.append(rankFIS.output['rank'])

print(rankFIS_Results[0])

sort_index = numpy.argsort(rankFIS_Results)

print('sorted files are')
print(sort_index)
print("The files in order of importance are: ")
for i in range(66):
  print(fileNames[sort_index[65-i]])

import numpy

sort_index = numpy.argsort(cos_Sim_Query)

print('sorted files are')
print(sort_index)
print("The files in order of importance are: ")
for i in range(66):
  print(fileNames[sort_index[65-i]])